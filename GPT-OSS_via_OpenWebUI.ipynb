{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama & OpenWebUI in Google Colab\\n",
    "\\n",
    "Dieses Notizbuch richtet ein Ollama-Sprachmodell ein und macht es √ºber eine √∂ffentliche OpenWebUI-Schnittstelle zug√§nglich.\\n",
    "\\n",
    "**Anleitung:**\\n",
    "1. Stellen Sie sicher, dass Ihre Colab-Laufzeit auf eine GPU-Instanz eingestellt ist (`Laufzeit` > `Laufzeittyp √§ndern` > `T4 GPU`).\\n",
    "2. F√ºhren Sie die Zellen nacheinander aus. √úberpr√ºfen Sie nach jedem Schritt die Ausgaben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown F√ºhren Sie diese Zelle aus, um alle erforderlichen Abh√§ngigkeiten zu installieren.\\n",
    "#@markdown > ‚è≥ **Dies kann einige Minuten dauern.**\\n",
    "%%capture\\n",
    "print(\"‚è≥ Installation von Ollama, OpenWebUI und ngrok wird gestartet...\")\\n",
    "!pip install open-webui pyngrok nest-asyncio\\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\\n",
    "print(\"‚úÖ Installation abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installationspr√ºfung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown F√ºhren Sie diese Zelle aus, um zu √ºberpr√ºfen, ob Ollama korrekt installiert wurde.\\n",
    "import os\n",
    "print(\"--- √úberpr√ºfung des Installationspfads ---\")\n",
    "path = !which ollama\n",
    "if path:\n",
    "    print(f\"‚úÖ Ollama gefunden in: {path[0]}\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama wurde nicht im Systempfad gefunden.\")\n",
    "\n",
    "print(\"\\n--- √úberpr√ºfung der Version ---\")\n",
    "version = !ollama --version\n",
    "if version:\n",
    "    print(f\"‚úÖ {version[0]}\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama-Version konnte nicht abgerufen werden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Systemressourcen pr√ºfen\\n",
    "\\n",
    "F√ºhren Sie diese Zelle aus, um den verf√ºgbaren Speicherplatz und Arbeitsspeicher zu √ºberpr√ºfen. Gro√üe Modelle ben√∂tigen viel von beidem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Festplattenspeicher ---\")\n",
    "!df -h | grep -E 'Filesystem|overlay'\n",
    "print(\"\\n--- Arbeitsspeicher (RAM) ---\")\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Konfiguration und Start\\n",
    "\\n",
    "Geben Sie hier Ihre Konfiguration ein.\\n",
    "\\n",
    "üí° **Hinweis zu den Modellen:** Gro√üe Modelle wie `gpt-oss:20b` (ca. 12 GB) oder `llama3:70b` (ca. 40 GB) ben√∂tigen extrem viel RAM und Speicherplatz. **Das Standardmodell `llama3:8b` (ca. 5 GB) ist eine gute Wahl f√ºr Standard-Colab-Instanzen.** Wenn Sie auf Schwierigkeiten sto√üen, versuchen Sie es mit einem noch kleineren Modell wie `phi3` (ca. 2.5 GB).\\n",
    "\\n",
    "‚ö†Ô∏è **Sicherheitswarnung:** Das direkte Einf√ºgen von geheimen Schl√ºsseln wird nicht empfohlen. Die sicherste Methode ist die Verwendung von **Colab Secrets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ---",
    "#@markdown ### Konfiguration\\n",
    "#@markdown Geben Sie das Ollama-Modell an, das Sie verwenden m√∂chten.\\n",
    "MODEL_NAME = \"llama3:8b\" #@param {type:\"string\"}\\n",
    "#@markdown Geben Sie Ihren ngrok Authtoken ein.\\n",
    "NGROK_AUTHTOKEN = \"\"  #@param {type:\"string\"}\\n",
    "#@markdown ---",
    "\\n",
    "import os\\n",
    "import asyncio\\n",
    "import nest_asyncio\\n",
    "from pyngrok import ngrok\\n",
    "import urllib.request\\n",
    "\\n",
    "nest_asyncio.apply()\\n",
    "\\n",
    "async def wait_for_service(url, timeout=120):\\n",
    "    print(f\"‚è≥ √úberpr√ºfe die Erreichbarkeit von {url} (Timeout: {timeout}s)...\")\\n",
    "    start_time = asyncio.get_event_loop().time()\\n",
    "    while True:\\n",
    "        try:\\n",
    "            with urllib.request.urlopen(url, timeout=5) as response:\\n",
    "                if response.status >= 200 and response.status < 300:\\n",
    "                    print(f\"‚úÖ Dienst unter {url} ist erreichbar.\")\\n",
    "                    return True\\n",
    "        except Exception:\\n",
    "            pass\\n",
    "        if (asyncio.get_event_loop().time() - start_time) > timeout:\\n",
    "            print(f\"‚ùå Zeit√ºberschreitung beim Warten auf {url}.\")\\n",
    "            return False\\n",
    "        await asyncio.sleep(5)\\n",
    "\\n",
    "async def run_ollama():\\n",
    "    if NGROK_AUTHTOKEN:\\n",
    "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\\n",
    "        print(\"‚úÖ ngrok Authtoken konfiguriert.\")\\n",
    "    else:\\n",
    "        print(\"‚ö†Ô∏è Kein ngrok Authtoken angegeben. Tunnel k√∂nnte fehlschlagen.\")\\n",
    "\\n",
    "    with open('ollama.log', 'w') as f: f.write('')\\n",
    "    with open('webui.log', 'w') as f: f.write('')\\n",
    "    with open('pull.log', 'w') as f: f.write('')\\n",
    "\\n",
    "    print(\"‚è≥ Starte Ollama-Server...\")\\n",
    "    ollama_process = await asyncio.create_subprocess_shell('ollama serve > ollama.log 2>&1')\\n",
    "    ollama_ready = await wait_for_service(\"http://127.0.0.1:11434\")\\n",
    "    if not ollama_ready:\\n",
    "        print(\"‚ùå Ollama-Server wurde nicht rechtzeitig erreichbar. Pr√ºfe ollama.log.\")\\n",
    "        return\\n",
    "\\n",
    "    print(f\"‚è≥ Lade das Modell '{MODEL_NAME}' herunter. Dies kann einige Zeit dauern...\")\\n",
    "    pull_process = await asyncio.create_subprocess_shell(f'ollama pull {MODEL_NAME} > pull.log 2>&1')\\n",
    "    await pull_process.wait()\\n",
    "    if pull_process.returncode != 0:\\n",
    "        print(\"‚ùå Fehler beim Herunterladen. Schau in pull.log und ollama.log.\")\\n",
    "        return\\n",
    "    print(f\"‚úÖ Modell '{MODEL_NAME}' erfolgreich heruntergeladen!\")\\n",
    "\\n",
    "    print(\"‚è≥ Starte OpenWebUI-Server...\")\\n",
    "    os.environ['OLLAMA_BASE_URL'] = 'http://127.0.0.1:11434'\\n",
    "    webui_process = await asyncio.create_subprocess_shell('open-webui --host 0.0.0.0 --port 8080 > webui.log 2>&1')\\n",
    "\\n",
    "    server_ready = await wait_for_service(\"http://127.0.0.1:8080\")\\n",
    "    if server_ready:\\n",
    "        try:\\n",
    "            public_url = ngrok.connect(8080)\\n",
    "            print(f\"\\n‚úÖ OpenWebUI ist bereit! Link: {public_url}\")\\n",
    "        except Exception as e:\\n",
    "            print(f'\\n‚ùå Fehler beim Starten von ngrok: {e}')\\n",
    "    else:\\n",
    "        print(\"\\n‚ùå OpenWebUI konnte nicht gestartet werden. Siehe webui.log\")\\n",
    "\\n",
    "    print(\"\\n‚ÑπÔ∏è Warte auf das Beenden von OpenWebUI (die Zelle bleibt offen).\")\\n",
    "    await webui_process.wait()\\n",
    "\\n",
    "asyncio.run(run_ollama())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 4. Fehlerbehebung\\n",
    "\\n",
    "Wenn Sie Probleme haben, f√ºhren Sie diese Zelle aus, um die Log-Dateien anzuzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- OLLAMA LOG (ollama.log) ---\")\n",
    "with open('ollama.log', 'r') as f:\n",
    "    print(f.read())\n",
    "\\n",
    "print(\"\\n--- MODEL PULL LOG (pull.log) ---\")\n",
    "with open('pull.log', 'r') as f:\n",
    "    print(f.read())\n",
    "\\n",
    "print(\"\\n--- OPENWEBUI LOG (webui.log) ---\")\n",
    "with open('webui.log', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}