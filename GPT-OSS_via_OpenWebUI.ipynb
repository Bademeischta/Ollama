{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama & OpenWebUI in Google Colab\\n",
    "\\n",
    "Dieses Notizbuch richtet ein Ollama-Sprachmodell ein und macht es √ºber eine √∂ffentliche OpenWebUI-Schnittstelle zug√§nglich.\\n",
    "\\n",
    "**Anleitung:**\\n",
    "1. Stellen Sie sicher, dass Ihre Colab-Laufzeit auf eine GPU-Instanz eingestellt ist (`Laufzeit` > `Laufzeittyp √§ndern` > `T4 GPU`).\\n",
    "2. F√ºgen Sie in Zelle 2 Ihren `ngrok`-Token und das gew√ºnschte Modell ein.\\n",
    "3. F√ºhren Sie die Zellen nacheinander aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "1. Installation"
   },
   "outputs": [],
   "source": [
    "#@markdown F√ºhren Sie diese Zelle aus, um alle erforderlichen Abh√§ngigkeiten zu installieren.\\n",
    "#@markdown > ‚è≥ **Dies kann einige Minuten dauern.**\\n",
    "%%capture\\n",
    "print(\"‚è≥ Installation von Ollama, OpenWebUI und ngrok wird gestartet...\")\\n",
    "!pip install open-webui pyngrok nest-asyncio\\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\\n",
    "print(\"‚úÖ Installation abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfiguration und Start\\n",
    "\\n",
    "Geben Sie hier Ihre Konfiguration ein.\\n",
    "\\n",
    "üí° **Hinweis zu den Modellen:** Gro√üe Modelle wie `gpt-oss:20b` (ca. 12 GB) ben√∂tigen viel RAM und Speicherplatz. Auf Standard-Colab-Instanzen kann dies zu Problemen f√ºhren. Wenn Sie auf Schwierigkeiten sto√üen, versuchen Sie es mit einem kleineren Modell wie `llama3:8b` (ca. 5 GB) oder `phi3` (ca. 2.5 GB).\\n",
    "\\n",
    "‚ö†Ô∏è **Sicherheitswarnung:** Das direkte Einf√ºgen von geheimen Schl√ºsseln (wie Ihrem `ngrok`-Token) wird nicht empfohlen. Die sicherste Methode ist die Verwendung von **Colab Secrets** (Schl√ºssel-Symbol in der Seitenleiste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ---",
    "#@markdown ### Konfiguration\\n",
    "#@markdown Geben Sie das Ollama-Modell an, das Sie verwenden m√∂chten.\\n",
    "MODEL_NAME = \"gpt-oss:20b\" #@param {type:\"string\"}\\n",
    "#@markdown Geben Sie Ihren ngrok Authtoken ein (finden Sie unter [dashboard.ngrok.com](https://dashboard.ngrok.com/get-started/your-authtoken)).\\n",
    "NGROK_AUTHTOKEN = \"\"  #@param {type:\"string\"}\\n",
    "#@markdown ---",
    "\\n",
    "import os\\n",
    "import asyncio\\n",
    "import nest_asyncio\\n",
    "from pyngrok import ngrok\\n",
    "import urllib.request\\n",
    "\\n",
    "nest_asyncio.apply()\\n",
    "\\n",
    "async def wait_for_service(url, timeout=180):\\n",
    "    print(f\"‚è≥ √úberpr√ºfe die Erreichbarkeit von {url} (Timeout: {timeout}s)...\")\\n",
    "    start_time = asyncio.get_event_loop().time()\\n",
    "    while True:\\n",
    "        try:\\n",
    "            with urllib.request.urlopen(url, timeout=5) as response:\\n",
    "                if response.status >= 200 and response.status < 300:\\n",
    "                    print(f\"‚úÖ Dienst unter {url} ist erreichbar.\")\\n",
    "                    return True\\n",
    "        except Exception:\\n",
    "            pass\\n",
    "        if (asyncio.get_event_loop().time() - start_time) > timeout:\\n",
    "            print(f\"‚ùå Zeit√ºberschreitung beim Warten auf {url}.\")\\n",
    "            return False\\n",
    "        await asyncio.sleep(5)\\n",
    "\\n",
    "async def run_ollama():\\n",
    "    if NGROK_AUTHTOKEN:\\n",
    "        ngrok.set_auth_token(NGROK_AUTHTOKEN)\\n",
    "        print(\"‚úÖ ngrok Authtoken konfiguriert.\")\\n",
    "    else:\\n",
    "        print(\"‚ö†Ô∏è Kein ngrok Authtoken angegeben. Tunnel k√∂nnte fehlschlagen.\")\\n",
    "\\n",
    "    with open('ollama.log', 'w') as f: f.write('')\\n",
    "    with open('webui.log', 'w') as f: f.write('')\\n",
    "\\n",
    "    print(\"‚è≥ Starte Ollama-Server...\")\\n",
    "    ollama_process = await asyncio.create_subprocess_shell('ollama serve > ollama.log 2>&1')\\n",
    "    await asyncio.sleep(5)\\n",
    "\\n",
    "    print(f\"‚è≥ Lade das Modell '{MODEL_NAME}' herunter. Dies kann einige Zeit dauern...\")\\n",
    "    pull_process = await asyncio.create_subprocess_shell(f'ollama pull {MODEL_NAME}')\\n",
    "    await pull_process.wait()\\n",
    "    if pull_process.returncode != 0:\\n",
    "        print(f\"‚ùå Fehler beim Herunterladen des Modells '{MODEL_NAME}'. √úberpr√ºfen Sie den Modellnamen und die ollama.log.\")\\n",
    "        return\\n",
    "    print(f\"‚úÖ Modell '{MODEL_NAME}' erfolgreich heruntergeladen!\")\\n",
    "\\n",
    "    print(\"‚è≥ Starte OpenWebUI-Server...\")\\n",
    "    os.environ['OLLAMA_BASE_URL'] = 'http://127.0.0.1:11434'\\n",
    "    webui_process = await asyncio.create_subprocess_shell('open-webui --host 0.0.0.0 --port 8080 > webui.log 2>&1')\\n",
    "\\n",
    "    server_ready = await wait_for_service(\"http://127.0.0.1:8080\")\\n",
    "\\n",
    "    if server_ready:\\n",
    "        try:\\n",
    "            public_url = ngrok.connect(8080)\\n",
    "            print(f\"\\n‚úÖ OpenWebUI ist bereit! √ñffnen Sie diesen Link in Ihrem Browser:\\n{public_url}\")\\n",
    "        except Exception as e:\\n",
    "            print(f'\\n‚ùå Fehler beim Starten von ngrok: {e}')\\n",
    "            print(\"√úberpr√ºfen Sie den Authtoken und die Log-Dateien.\")\\n",
    "    else:\\n",
    "        print(\"\\n‚ùå OpenWebUI konnte nicht gestartet werden. F√ºhren Sie die n√§chste Zelle aus, um die Logs zu √ºberpr√ºfen.\")\\n",
    "    \\n",
    "    print(\"\\n‚ÑπÔ∏è Die Server laufen im Hintergrund. Diese Zelle muss aktiv bleiben, um die Verbindung aufrechtzuerhalten.\")\\n",
    "\\n",
    "asyncio.run(run_ollama())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 3. Fehlerbehebung\\n",
    "\\n",
    "Wenn Sie Probleme haben (z.B. eine wei√üe Seite sehen), f√ºhren Sie die folgende Zelle aus, um die Log-Dateien der Server anzuzeigen. Dies kann Aufschluss √ºber m√∂gliche Fehler geben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- OLLAMA LOG (ollama.log) ---\")\n",
    "with open('ollama.log', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\n--- OPENWEBUI LOG (webui.log) ---\")\n",
    "with open('webui.log', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}